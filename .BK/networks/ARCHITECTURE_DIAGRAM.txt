# Transformer Autoencoder Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    TRANSFORMER AUTOENCODER ARCHITECTURE                       │
│                         AutoEncoder_TransformerV2_0                          │
└─────────────────────────────────────────────────────────────────────────────┘

INPUT: Grayscale Images
├─ 201x201 (Square)
├─ 1280x152 (Wide)
└─ Xx152 (Variable width)

                                    ↓ forward()

┌─────────────────────────────────────────────────────────────────────────────┐
│                               PREPROCESSING                                   │
└─────────────────────────────────────────────────────────────────────────────┘

Input Tensor: (Batch, 1, Height, Width)
                    ↓
              Flatten to (Batch, H*W)


┌─────────────────────────────────────────────────────────────────────────────┐
│                                 ENCODER                                       │
│                            Encoder_Transformer                                │
└─────────────────────────────────────────────────────────────────────────────┘

(Batch, H*W)
      ↓
┌─────────────────────────────────────┐
│   Linear Projection                 │
│   input_dim → latent_dim           │
└─────────────────────────────────────┘
      ↓
(Batch, 1, latent_dim)
      ↓
┌─────────────────────────────────────┐
│   Learnable Positional Encoding     │
│   (Initialized with sinusoidal)     │
└─────────────────────────────────────┘
      ↓
(Batch, 1, latent_dim)
      ↓
┌─────────────────────────────────────┐
│   Transformer Block 1               │
│   ┌─────────────────────────────┐   │
│   │ Layer Norm                  │   │
│   │ Multi-Head Self-Attention   │   │  ◄─── Optional: Flash Attention 2
│   │ (Extract attention weights) │   │  ◄─── For visualization
│   │ Residual Connection         │   │
│   │ Layer Norm                  │   │
│   │ MLP (4x expansion)          │   │
│   │ Residual Connection         │   │
│   └─────────────────────────────┘   │
└─────────────────────────────────────┘
      ↓
┌─────────────────────────────────────┐
│   Transformer Block 2...N           │
│   (Same structure)                  │  ◄─── Optional: Gradient Checkpointing
└─────────────────────────────────────┘
      ↓
(Batch, 1, latent_dim)
      ↓
┌─────────────────────────────────────┐
│   Final Layer Norm                  │
└─────────────────────────────────────┘
      ↓
(Batch, 1, latent_dim)
      ↓
┌─────────────────────────────────────┐
│   Squeeze (remove sequence dim)     │
└─────────────────────────────────────┘
      ↓
(Batch, latent_dim) ◄────────────────────── LATENT EMBEDDING (for LSTM)


┌─────────────────────────────────────────────────────────────────────────────┐
│                                 DECODER                                       │
│                            Decoder_Transformer                                │
└─────────────────────────────────────────────────────────────────────────────┘

(Batch, latent_dim)
      ↓
┌─────────────────────────────────────┐
│   Linear Expansion                  │
│   latent_dim → latent_dim          │
└─────────────────────────────────────┘
      ↓
┌─────────────────────────────────────┐
│   Unsqueeze (add sequence dim)      │
└─────────────────────────────────────┘
      ↓
(Batch, 1, latent_dim)
      ↓
┌─────────────────────────────────────┐
│   Learnable Positional Encoding     │
└─────────────────────────────────────┘
      ↓
┌─────────────────────────────────────┐
│   Transformer Block 1               │
│   (Same structure as encoder)       │
└─────────────────────────────────────┘
      ↓
┌─────────────────────────────────────┐
│   Transformer Block 2...N           │
└─────────────────────────────────────┘
      ↓
(Batch, 1, latent_dim)
      ↓
┌─────────────────────────────────────┐
│   Final Layer Norm                  │
└─────────────────────────────────────┘
      ↓
┌─────────────────────────────────────┐
│   Linear Projection                 │
│   latent_dim → H*W                 │
└─────────────────────────────────────┘
      ↓
(Batch, 1, H*W)
      ↓
┌─────────────────────────────────────┐
│   Squeeze & Reshape                 │
└─────────────────────────────────────┘
      ↓
(Batch, 1, Height, Width)


┌─────────────────────────────────────────────────────────────────────────────┐
│                                 OUTPUT                                        │
└─────────────────────────────────────────────────────────────────────────────┘

Reconstruction: (Batch, 1, Height, Width)
Attention Maps: List of (Batch, seq_len, seq_len) for each layer


═══════════════════════════════════════════════════════════════════════════════
                           MULTI-HEAD ATTENTION DETAIL
═══════════════════════════════════════════════════════════════════════════════

Input: (Batch, seq_len, embed_dim)
      ↓
┌─────────────────────────────────────────────────────────────┐
│  Single Linear Projection (Efficient)                       │
│  embed_dim → 3 * embed_dim                                 │
│  Split into Q, K, V                                        │
└─────────────────────────────────────────────────────────────┘
      ↓
Q, K, V: (Batch, num_heads, seq_len, head_dim)
      ↓
┌─────────────────────────────────────────────────────────────┐
│  Flash Attention 2 (if available)                          │
│  OR                                                         │
│  Standard Scaled Dot-Product Attention                     │
│                                                             │
│  Attention = softmax(QK^T / √d_k)V                        │
│                                                             │
│  ◄─── Attention weights extracted here (if requested)      │
└─────────────────────────────────────────────────────────────┘
      ↓
(Batch, num_heads, seq_len, head_dim)
      ↓
┌─────────────────────────────────────────────────────────────┐
│  Transpose & Reshape                                        │
└─────────────────────────────────────────────────────────────┘
      ↓
(Batch, seq_len, embed_dim)
      ↓
┌─────────────────────────────────────────────────────────────┐
│  Output Projection                                          │
└─────────────────────────────────────────────────────────────┘
      ↓
Output: (Batch, seq_len, embed_dim)


═══════════════════════════════════════════════════════════════════════════════
                              CONFIGURATION PRESETS
═══════════════════════════════════════════════════════════════════════════════

┌──────────┬─────────────┬───────┬─────────────┬────────────┬──────────────────┐
│  Preset  │ Latent Dim  │ Heads │ Enc Layers  │ Dec Layers │   Parameters     │
├──────────┼─────────────┼───────┼─────────────┼────────────┼──────────────────┤
│  tiny    │     128     │   4   │      3      │     3      │    ~500K         │
│  small   │     256     │   8   │      4      │     4      │    ~2M           │
│  medium  │     512     │   8   │      6      │     6      │    ~8M           │
│  large   │    1024     │  16   │      8      │     8      │    ~32M          │
│  xlarge  │    2048     │  16   │     12      │    12      │    ~128M         │
└──────────┴─────────────┴───────┴─────────────┴────────────┴──────────────────┘


═══════════════════════════════════════════════════════════════════════════════
                                 KEY METHODS
═══════════════════════════════════════════════════════════════════════════════

1. forward(x, return_attention=False)
   ├─ Input: (Batch, 1, H, W)
   ├─ Output: Reconstruction (Batch, 1, H, W)
   └─ Optional: Attention info dict

2. Embedding(x)
   ├─ Input: (Batch, 1, H, W)
   └─ Output: Latent embeddings (Batch, latent_dim)
   └─ Use for LSTM integration

3. get_attention_maps(x)
   ├─ Input: (Batch, 1, H, W)
   └─ Output: Dict with layer attention matrices
       ├─ 'layer_0': (Batch, seq_len, seq_len)
       ├─ 'layer_1': (Batch, seq_len, seq_len)
       ├─ ...
       └─ 'average': (Batch, seq_len, seq_len)

4. get_config()
   └─ Output: Dict with model configuration


═══════════════════════════════════════════════════════════════════════════════
                            GPU OPTIMIZATION FLOW
═══════════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────────┐
│ 1. Flash Attention 2 (use_flash_attention=True)                             │
│    ├─ PyTorch 2.0+ F.scaled_dot_product_attention()                        │
│    ├─ 2-3x faster than standard attention                                  │
│    └─ Automatic memory-efficient implementation                            │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ 2. Mixed Precision Training (torch.cuda.amp)                                │
│    ├─ FP16 for forward/backward                                            │
│    ├─ FP32 for parameter updates                                           │
│    ├─ ~50% memory reduction                                                │
│    └─ ~30-40% speed increase                                               │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ 3. Gradient Checkpointing (use_gradient_checkpointing=True)                 │
│    ├─ Recompute activations during backward                                │
│    ├─ ~30-40% memory reduction                                             │
│    └─ ~10-15% speed cost                                                   │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ 4. Torch Compile (torch.compile)                                            │
│    ├─ PyTorch 2.0+ graph compilation                                       │
│    ├─ Up to 2x speedup                                                     │
│    └─ One-line addition: model = torch.compile(model)                      │
└─────────────────────────────────────────────────────────────────────────────┘


═══════════════════════════════════════════════════════════════════════════════
                              INTEGRATION EXAMPLE
═══════════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────────┐
│                           IMAGE SEQUENCE → LSTM                              │
└─────────────────────────────────────────────────────────────────────────────┘

Image Sequence: (Batch, Seq_Len, 1, Height, Width)
                            ↓
        ┌─────────────────────────────────────┐
        │  For each timestep t:               │
        │    frame = sequence[:, t]           │
        │    embedding = autoencoder(frame)   │
        └─────────────────────────────────────┘
                            ↓
Embeddings: (Batch, Seq_Len, latent_dim)
                            ↓
        ┌─────────────────────────────────────┐
        │  LSTM Network                       │
        │  (hidden_dim, num_layers)          │
        └─────────────────────────────────────┘
                            ↓
LSTM Output: (Batch, Seq_Len, hidden_dim)
                            ↓
            Final Predictions


═══════════════════════════════════════════════════════════════════════════════
                            ATTENTION VISUALIZATION
═══════════════════════════════════════════════════════════════════════════════

Input Image → Encoder → Attention Weights (seq_len × seq_len)

Interpretation:
├─ High attention weight → Strong correlation between positions
├─ Diagonal values → Self-attention (position attending to itself)
├─ Off-diagonal → Long-range dependencies
└─ Different layers capture different patterns
    ├─ Early layers: Local features
    ├─ Middle layers: Medium-range patterns
    └─ Late layers: Global structure

Visualization Matrix:
    Query Position (rows)
         ↓
    ┌────────────────┐
    │ █░░░░░░░░░░░░░ │ ← Key Position (columns)
    │ ░█░░░░░░░░░░░░ │
    │ ░░█░░░░░░░░░░░ │
    │ ░░░█░░░░░░░░░░ │
    │ ░░░░█░░░░░░░░░ │
    │ ░░░░░█░░░░░░░░ │
    └────────────────┘
    Dark = High attention
    Light = Low attention


═══════════════════════════════════════════════════════════════════════════════
                              MEMORY USAGE (8GB GPU)
═══════════════════════════════════════════════════════════════════════════════

201x201 Input, Medium Preset (512 latent_dim):

FP32 Training:
├─ Model: ~32 MB
├─ Activations (batch=16): ~500 MB
├─ Gradients: ~32 MB
├─ Optimizer states (Adam): ~64 MB
└─ Total: ~630 MB ✓ Fits easily

Mixed Precision (FP16):
├─ Model: ~16 MB (50% reduction)
├─ Activations (batch=32): ~500 MB (2x batch size!)
├─ Gradients: ~16 MB
├─ Optimizer states: ~48 MB
└─ Total: ~580 MB ✓ 2x throughput

With Gradient Checkpointing:
├─ Model: ~32 MB
├─ Activations (batch=32): ~300 MB (40% reduction)
├─ Gradients: ~32 MB
├─ Optimizer states: ~64 MB
└─ Total: ~430 MB ✓ Even larger batches possible


═══════════════════════════════════════════════════════════════════════════════
                                 QUICK START
═══════════════════════════════════════════════════════════════════════════════

from networks.AutoEncoder_TransformerV2_0 import create_autoencoder
import torch

# Create model
model = create_autoencoder(
    input_size=(201, 201),
    preset='medium',
    use_flash_attention=True
).cuda()

# Forward pass
x = torch.randn(16, 1, 201, 201).cuda()
reconstruction, _ = model(x)

# Extract embeddings
embeddings = model.Embedding(x)

# Visualize attention
attention_maps = model.get_attention_maps(x[:1])

═══════════════════════════════════════════════════════════════════════════════
```
