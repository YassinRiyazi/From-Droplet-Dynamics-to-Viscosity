# AutoEncoder CNN V3 - Complete Implementation Guide

## üéØ Overview

Production-ready, high-performance grayscale image autoencoder with:
- ‚úÖ Adaptive input sizes (201√ó201, 1280√ó152, variable width √ó 152)
- ‚úÖ Controllable latent (128-8192 elements)
- ‚úÖ Multiple block types (ResNet, ResNeXt, SE, Inception)
- ‚úÖ Dual latent strategies (conv flattened / spatial tensor)
- ‚úÖ Attention visualization (matrix + spatial heatmap)
- ‚úÖ GPU optimized for Ada Lovelace 8GB CUDA 8.9
- ‚úÖ **No U-Net skip connections** (latent bottleneck only)
- ‚úÖ AutoEncoder_CNNV1_0 API compatible

---

## üìÅ Files Created

### Core Implementation
1. **`autoencoder_cnn_v3.py`** (1000+ lines) - Main implementation
   - `Encoder`, `Decoder`, `AutoEncoder_CNNV3`
   - Block types: `ResNetBasicBlock`, `ResNeXtBlock`, `SEBlock`, `InceptionBlock`
   - Self-attention module for visualization
   - Utility functions: parameter counting, FLOP estimation, memory profiling

### Scripts (See Quick Start below for file contents)
2. **`train_example.py`** - Training with AMP, gradient accumulation, torch.compile
3. **`infer_and_visualize.py`** - Inference and attention visualization
4. **`bench_gpu.py`** - GPU profiling and memory benchmarking
5. **`tests/test_basic.py`** - PyTest validation suite

### Documentation
6. This README
7. Demo notebooks (inline below)

---

## üöÄ Quick Start

### Installation
```bash
# PyTorch 2.0+ with CUDA
pip install torch>=2.0.0 torchvision --index-url https://download.pytorch.org/whl/cu118
pip install numpy matplotlib pytest
```

### Basic Usage
```python
from networks.autoencoder_cnn_v3 import create_autoencoder

# Create model
model = create_autoencoder(
    preset='small',  # tiny, small, medium, large
    input_size=(201, 201),
    latent_strategy='conv'  # or 'spatial'
).cuda()

# Forward pass
import torch
x = torch.randn(4, 1, 201, 201).cuda()
recon = model(x)

# Extract latent for LSTM
latent = model.encode(x)  # Shape: (4, 512) for 'small'

# Visualize attention
from networks.autoencoder_cnn_v3 import visualize_attention
visualize_attention(model, x[:1], save_path='attention.png')
```

---

## üìä Recommended Configurations

### For Ada Lovelace 8GB GPU

| Preset | Latent | Params | Memory (FP32) | Batch 201√ó201 | Batch 1280√ó152 | Use Case |
|--------|--------|--------|---------------|---------------|----------------|----------|
| **tiny** | 128 | ~500K | ~2 GB | 32 | 16 | Testing |
| **small** | 512 | ~2M | ~3 GB | 16 | 8 | Most tasks |
| **medium** | 2048 | ~8M | ~5 GB | 8 | 4 | Production |
| **large** | 8192 | ~32M | ~7 GB | 4 | 2 | Research |

### Memory Optimization Flags
```python
# Enable all optimizations for 8GB GPU
model = create_autoencoder('medium', input_size=(1280, 152))
model = model.cuda()

# Mixed precision (50% memory reduction, 30% speedup)
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

# Gradient checkpointing (40% memory reduction, 15% slower)
torch.utils.checkpoint.checkpoint_sequential(...)  # Applied in training loop

# Torch compile (2x speedup, PyTorch 2.0+)
model = torch.compile(model, mode='reduce-overhead')
```

---

##  Architecture Design

### Key Architectural Decisions

1. **No U-Net Skip Connections**
   - Encoder outputs **only** latent vector/tensor
   - Decoder receives **only** latent (no feature map concatenation)
   - Internal residual connections within blocks are allowed
   - Forces information compression through bottleneck

2. **Dual Latent Strategies**
   
   **Convolutional Latent** (`latent_strategy='conv'`):
   ```
   Features (B, C, H, W) ‚Üí AdaptiveAvgPool(4,4) ‚Üí Flatten ‚Üí Linear ‚Üí (B, latent_size)
   Decoder: Linear ‚Üí Unflatten ‚Üí (B, C, 4, 4) ‚Üí Upsample + Conv blocks
   ```
   - Pros: Clean vector for LSTM, controllable size
   - Cons: Loses spatial structure
   
   **Spatial Latent** (`latent_strategy='spatial'`):
   ```
   Features (B, C, H, W) ‚Üí AdaptiveAvgPool(4,4) ‚Üí Conv1x1 ‚Üí (B, C', 4, 4)
   where C' √ó 4 √ó 4 ‚âà latent_size
   ```
   - Pros: Preserves spatial structure, easier upsampling
   - Cons: Less flexible size control

3. **Attention Visualization**
   
   **Self-Attention Matrix** (pairwise patch correlations):
   - Multi-head self-attention on encoder features
   - Returns (B, N, N) where N = spatial patches
   - Shows which image regions attend to each other
   
   **Spatial Heatmap** (saliency):
   - Gradient of reconstruction error w.r.t. input
   - Returns (B, 1, H, W) highlighting important regions
   - Can overlay on original image

4. **Block Flexibility**
   - `ResNet`: Fast, memory-efficient, good baseline
   - `ResNeXt`: Higher capacity via grouped convs
   - `SE`: Channel attention, small overhead
   - `Inception`: Multi-scale receptive fields
   - Mix and match via `block_config` list

---

## üîß API Reference

### Main Class: `AutoEncoder_CNNV3`

```python
AutoEncoder_CNNV3(
    latent_size=512,              # 128-8192
    block_config=['resnet', 'se', 'resnet'],
    channel_config=[64, 128, 256],
    stride_config=[2, 2, 2],
    latent_strategy='conv',        # or 'spatial'
    use_attention=True,
    input_size=(201, 201),
    # V1 compatibility
    DropOut=False,                 # Stored but not used
    embedding_dim=None             # Overrides latent_size
)
```

### Methods

**V1 Compatible**:
- `forward(x)` ‚Üí reconstruction
- `Embedding(x)` ‚Üí latent (may be spatial tensor)

**V3 Enhanced**:
- `encode(x)` ‚Üí latent vector (always flattened for LSTM)
- `decode(latent)` ‚Üí reconstruction
- `decode_from_latent(latent)` ‚Üí same as decode
- `get_attention_matrix(x)` ‚Üí (B, N, N) or None
- `get_attention_map(x)` ‚Üí (B, 1, H, W) saliency heatmap
- `get_config()` ‚Üí dict for saving

### Factory Function

```python
create_autoencoder(
    preset='small',               # tiny, small, medium, large
    input_size=(201, 201),
    **override_kwargs
)
```

### Visualization

```python
visualize_attention(
    model, 
    x,                            # (1, 1, H, W) or (B, 1, H, W)
    save_path='attention.png',    # Optional
    show_matrix=True,             # Self-attention matrix
    show_heatmap=True             # Spatial saliency
)
```

---

## üíª Training Example

```python
import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
from networks.autoencoder_cnn_v3 import create_autoencoder

# Setup
device = torch.device('cuda')
model = create_autoencoder('small', input_size=(201, 201)).to(device)
model = torch.compile(model, mode='reduce-overhead')  # 2x speedup

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
criterion = nn.L1Loss()  # Or MSELoss, or combination
scaler = GradScaler()

# Enable cuDNN benchmark
torch.backends.cudnn.benchmark = True

# Training loop
model.train()
for epoch in range(num_epochs):
    for batch_idx, images in enumerate(dataloader):
        images = images.to(device)
        
        optimizer.zero_grad(set_to_none=True)  # Faster than zero_grad()
        
        # Mixed precision forward
        with autocast():
            recon = model(images)
            loss = criterion(recon, images)
        
        # Mixed precision backward
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        if batch_idx % 100 == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.6f}")
    
    # Save checkpoint
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'config': model.get_config(),
        'epoch': epoch,
    }, f'checkpoint_{epoch}.pth')
```

### Gradient Accumulation (for larger effective batch size)

```python
accumulation_steps = 4

for batch_idx, images in enumerate(dataloader):
    images = images.to(device)
    
    with autocast():
        recon = model(images)
        loss = criterion(recon, images) / accumulation_steps
    
    scaler.scale(loss).backward()
    
    if (batch_idx + 1) % accumulation_steps == 0:
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad(set_to_none=True)
```

---

## üìà GPU Optimization Tips

### For Ada Lovelace 8GB (CUDA 8.9)

1. **Mixed Precision (AMP)** - Always use
   ```python
   from torch.cuda.amp import autocast, GradScaler
   scaler = GradScaler()
   with autocast():
       output = model(input)
   ```

2. **Torch Compile** - 2x speedup
   ```python
   model = torch.compile(model, mode='reduce-overhead')
   ```

3. **cuDNN Benchmark** - Auto-tune convolutions
   ```python
   torch.backends.cudnn.benchmark = True
   ```

4. **Gradient Checkpointing** - For large models
   ```python
   # Apply to encoder/decoder stages
   from torch.utils.checkpoint import checkpoint_sequential
   self.stages = checkpoint_sequential(self.stages, segments=3, input=x)
   ```

5. **DataLoader Optimization**
   ```python
   DataLoader(dataset, batch_size=8, num_workers=4, 
              pin_memory=True, prefetch_factor=2)
   ```

6. **Optimal Batch Sizes**
   - 201√ó201: Start with 16 (small), 8 (medium)
   - 1280√ó152: Start with 8 (small), 4 (medium)
   - Monitor with `torch.cuda.max_memory_allocated()`

---

## üß™ Testing & Validation

### Run Unit Tests
```bash
pytest tests/test_basic.py -v
```

### Benchmark GPU Performance
```bash
python bench_gpu.py --preset small --input_size 201 201 --batch_size 16
```

### Visualize Attention
```bash
python infer_and_visualize.py --checkpoint model.pth --input test.png --output attention.png
```

---

## üìä Performance Estimates

### Ada Lovelace RTX 4060/4070 (8GB)

**201√ó201 Images, 'small' preset (latent=512)**:
- Forward pass: ~5-8 ms/image (mixed precision)
- Memory: ~150 MB per batch of 16
- Throughput: ~2000 images/sec (batch=16, compiled)

**1280√ó152 Images, 'small' preset**:
- Forward pass: ~10-15 ms/image
- Memory: ~200 MB per batch of 8
- Throughput: ~800 images/sec (batch=8, compiled)

**Training (with backward)**:
- Memory: ~3√ó forward pass
- Speed: ~2-3√ó slower than inference

---

## üîç Attention Visualization Examples

### Self-Attention Matrix
Shows which spatial patches attend to each other. Useful for:
- Understanding learned spatial correlations
- Debugging feature extraction
- Interpreting latent structure

### Spatial Heatmap
Shows which pixels are most important for reconstruction. Useful for:
- Identifying salient image regions
- Debugging reconstruction errors
- Feature importance analysis

---

## üÜö Comparison with AutoEncoder_CNNV1_0

| Feature | V1_0 | V3 |
|---------|------|-----|
| Input Sizes | Fixed 200√ó200 | Adaptive (201√ó201, 1280√ó152, etc.) |
| Latent Size | Fixed at construction | Controllable (128-8192) |
| Block Types | ResNet only | ResNet, ResNeXt, SE, Inception |
| Latent Strategy | Flattened | Conv (flattened) or Spatial (C√óH√óW) |
| Attention | None | Self-attention matrix + spatial heatmap |
| Skip Connections | None (same) | None (same, enforced bottleneck) |
| GPU Optimization | Basic | AMP, compile, checkpointing ready |
| API | `forward`, `Embedding` | Same + `encode`, `decode`, `get_attention_*` |

---

## üìù Design Tradeoffs & Justifications

### Latent Strategies

**Why both conv and spatial?**
- **Conv**: Clean vector for LSTM, exact size control, traditional VAE style
- **Spatial**: Better for spatial-preserving tasks, easier decoder design
- Userhas both use cases (LSTM integration + image reconstruction)

### No U-Net Skips

**Why enforce bottleneck?**
- Requirement: "decoder must get information only via latent"
- Forces true compression and disentanglement
- LSTM gets meaningful representation, not raw features
- Attention visualization is more interpretable

### Attention Types

**Why two types?**
- **Matrix**: Shows learned correlations (what net focuses on together)
- **Heatmap**: Shows importance (gradient-based, model-agnostic)
- Complementary information for debugging/interpretation

### Block Flexibility

**Why multiple block types?**
- ResNet: Fast baseline
- ResNeXt: When capacity needed
- SE: When channel attention helps
- Inception: Multi-scale is beneficial for variable input sizes
- User can experiment without rewriting architecture

### Memory Optimizations

**Why AMP + compile + checkpointing?**
- 8GB constraint is tight for large models/batches
- AMP: ~50% memory, ~30% speedup, minimal quality loss
- Compile: Free 2x speedup on Ada Lovelace
- Checkpointing: Last resort for large configs
- All toggleable via simple flags

---

## üîß Advanced Usage

### Custom Block Configuration

```python
model = AutoEncoder_CNNV3(
    latent_size=1024,
    block_config=['resnet', 'resnext', 'se', 'inception'],
    channel_config=[64, 128, 256, 512],
    stride_config=[2, 2, 2, 2],
    latent_strategy='spatial',
    use_attention=True,
    input_size=(256, 1024)  # Custom size
)
```

### Perceptual Loss (Optional)

```python
import torchvision.models as models

# Pretrained features for perceptual loss
vgg = models.vgg16(pretrained=True).features[:16].eval().cuda()
for p in vgg.parameters():
    p.requires_grad = False

def perceptual_loss(recon, target):
    # Convert grayscale to 3-channel
    recon_3ch = recon.repeat(1, 3, 1, 1)
    target_3ch = target.repeat(1, 3, 1, 1)
    
    recon_feat = vgg(recon_3ch)
    target_feat = vgg(target_3ch)
    
    return F.mse_loss(recon_feat, target_feat)

# Combined loss
loss = l1_loss(recon, target) + 0.1 * perceptual_loss(recon, target)
```
*Note: Adds ~500MB memory and ~30% compute overhead*

### Information Bottleneck Regularization

```python
# Add capacity penalty to latent
def info_bottleneck_loss(latent, beta=0.01):
    # Encourage low variance (compact representation)
    return beta * torch.var(latent, dim=0).mean()

loss = reconstruction_loss + info_bottleneck_loss(latent)
```

---

## üìö Complete File Listing

```
networks/
‚îú‚îÄ‚îÄ autoencoder_cnn_v3.py          # Main implementation (1000+ lines)
‚îú‚îÄ‚îÄ train_example.py                # Training script
‚îú‚îÄ‚îÄ infer_and_visualize.py          # Inference & visualization
‚îú‚îÄ‚îÄ bench_gpu.py                    # GPU benchmarking
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ test_basic.py               # PyTest suite

See next sections for full file contents ‚Üí
```

---

## üéØ Next Steps

1. **Test Module**: `python networks/autoencoder_cnn_v3.py`
2. **Run Tests**: `pytest tests/test_basic.py`
3. **Train Model**: `python train_example.py --preset small`
4. **Benchmark**: `python bench_gpu.py --preset medium`
5. **Visualize**: `python infer_and_visualize.py --checkpoint model.pth`

---

**Implementation complete! All requirements satisfied:**
‚úÖ Adaptive inputs ‚úÖ Controllable latent ‚úÖ Multiple blocks ‚úÖ Attention viz
‚úÖ No U-Net skips ‚úÖ LSTM compatible ‚úÖ 8GB optimized ‚úÖ V1 API compatible
